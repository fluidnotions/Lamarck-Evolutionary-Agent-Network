# **HVAS Mini Prototype - Technical Specification**

## **Project Overview**

A demonstration of Hierarchical Vector Agent System (HVAS) concepts using LangGraph, showcasing concurrent AI agents with individual RAG memory, parameter evolution, and real-time learning visualization.

---

## **1. System Architecture**

### **1.1 Core Components**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         LangGraph Orchestrator          ‚îÇ
‚îÇ         (Streaming + Async)             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ        ‚îÇ        ‚îÇ         ‚îÇ (Parallel Execution)
‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê
‚îÇIntro ‚îÇ ‚îÇBody  ‚îÇ ‚îÇConcl ‚îÇ ‚îÇEval  ‚îÇ
‚îÇAgent ‚îÇ ‚îÇAgent ‚îÇ ‚îÇAgent ‚îÇ ‚îÇNode  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ       ‚îÇ        ‚îÇ         ‚îÇ
    ‚ñº       ‚ñº        ‚ñº         ‚ñº
[ChromaDB] [ChromaDB] [ChromaDB] [Scores]
Collection Collection Collection  Cache
```

### **1.2 Data Flow**

1. **Input**: Topic string
2. **Parallel Processing**: Agents work concurrently with context passing
3. **Memory Retrieval**: Each agent queries its ChromaDB collection
4. **Generation**: LLM creates content informed by memories
5. **Evaluation**: Scoring system rates each section
6. **Memory Storage**: High-scoring outputs saved for future use
7. **Parameter Evolution**: Agents adjust temperature based on performance

---

## **2. Technical Implementation**

### **2.1 Project Structure**

```
hvas-mini/
‚îú‚îÄ‚îÄ .env                    # Configuration
‚îú‚îÄ‚îÄ requirements.txt        # Dependencies
‚îú‚îÄ‚îÄ main.py                # Entry point
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ agents.py          # Agent implementations
‚îÇ   ‚îú‚îÄ‚îÄ state.py           # State definitions
‚îÇ   ‚îú‚îÄ‚îÄ evaluation.py      # Scoring logic
‚îÇ   ‚îú‚îÄ‚îÄ memory.py          # RAG memory management
‚îÇ   ‚îú‚îÄ‚îÄ evolution.py       # Parameter evolution
‚îÇ   ‚îî‚îÄ‚îÄ visualization.py   # Stream visualization
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ memories/          # ChromaDB persistence
‚îî‚îÄ‚îÄ logs/
    ‚îî‚îÄ‚îÄ runs/              # Execution logs
```

### **2.2 Environment Configuration**

```bash
# .env
# LLM Configuration
ANTHROPIC_API_KEY=your_key_here
MODEL_NAME=claude-3-haiku-20240307
BASE_TEMPERATURE=0.7

# Memory Configuration
EMBEDDING_MODEL=all-MiniLM-L6-v2
MEMORY_SCORE_THRESHOLD=7.0
MAX_MEMORIES_RETRIEVE=3

# Evolution Configuration
ENABLE_PARAMETER_EVOLUTION=true
EVOLUTION_LEARNING_RATE=0.1
MIN_TEMPERATURE=0.5
MAX_TEMPERATURE=1.0

# Visualization
ENABLE_VISUALIZATION=true
SHOW_MEMORY_RETRIEVAL=true
SHOW_PARAMETER_CHANGES=true

# LangGraph Configuration
STREAM_MODE=values
RECURSION_LIMIT=10
```

### **2.3 Dependencies**

```txt
# requirements.txt
langgraph>=0.2.0
langchain>=0.3.0
langchain-anthropic>=0.2.0
chromadb>=0.5.0
sentence-transformers>=3.0.0
python-dotenv>=1.0.0
rich>=13.0.0  # For visualization
asyncio>=3.11
numpy>=1.24.0
pydantic>=2.0.0
```

---

## **3. Core Implementation**

### **3.1 State Management**

```python
# src/state.py
from typing import TypedDict, Dict, List, Optional
from pydantic import BaseModel, Field

class AgentMemory(BaseModel):
    """Individual agent memory record"""
    content: str
    topic: str
    score: float
    timestamp: str
    embeddings: Optional[List[float]] = None
    retrieval_count: int = 0

class BlogState(TypedDict):
    """Shared state for blog generation"""
    # Content
    topic: str
    intro: str
    body: str
    conclusion: str
    
    # Scores
    scores: Dict[str, float]
    
    # Memory & Evolution
    retrieved_memories: Dict[str, List[str]]
    parameter_updates: Dict[str, Dict[str, float]]
    
    # Metadata
    generation_id: str
    timestamp: str
    stream_logs: List[str]
```

### **3.2 Base Agent with Memory**

```python
# src/agents.py
from abc import ABC, abstractmethod
from typing import List, Dict, Optional
import chromadb
from sentence_transformers import SentenceTransformer
from langchain_anthropic import ChatAnthropic
from datetime import datetime
import os
from dotenv import load_dotenv
import json

load_dotenv()

class BaseAgent(ABC):
    """Base agent with RAG memory and parameter evolution"""
    
    def __init__(self, role: str, chroma_client: chromadb.Client):
        self.role = role
        self.llm = ChatAnthropic(
            model=os.getenv("MODEL_NAME", "claude-3-haiku-20240307")
        )
        
        # Memory setup
        self.memory = chroma_client.get_or_create_collection(f"{role}_memories")
        self.embedder = SentenceTransformer(os.getenv("EMBEDDING_MODEL"))
        
        # Evolutionary parameters
        self.parameters = {
            "temperature": float(os.getenv("BASE_TEMPERATURE", 0.7)),
            "score_history": [],
            "generation_count": 0
        }
        
        # Configuration
        self.enable_evolution = os.getenv("ENABLE_PARAMETER_EVOLUTION", "true").lower() == "true"
        self.score_threshold = float(os.getenv("MEMORY_SCORE_THRESHOLD", 7.0))
    
    async def __call__(self, state: BlogState) -> BlogState:
        """Execute agent with memory retrieval and generation"""
        
        # 1. Retrieve relevant memories
        memories = await self.retrieve_memories(state["topic"])
        state["retrieved_memories"][self.role] = [m['content'] for m in memories]
        
        # 2. Log retrieval for visualization
        if os.getenv("SHOW_MEMORY_RETRIEVAL", "true").lower() == "true":
            state["stream_logs"].append(
                f"[{self.role}] Retrieved {len(memories)} memories"
            )
        
        # 3. Generate content with current parameters
        self.llm.temperature = self.parameters["temperature"]
        content = await self.generate_content(state, memories)
        
        # 4. Store in state
        state[self.content_key] = content
        
        # 5. Prepare for memory storage
        self.pending_memory = {
            "content": content,
            "topic": state["topic"],
            "timestamp": datetime.now().isoformat()
        }
        
        return state
    
    async def retrieve_memories(self, topic: str) -> List[Dict]:
        """Retrieve similar high-scoring memories"""
        try:
            max_retrieve = int(os.getenv("MAX_MEMORIES_RETRIEVE", 3))
            
            # Query ChromaDB
            results = self.memory.query(
                query_texts=[topic],
                n_results=min(max_retrieve, self.memory.count()),
                where={"score": {"$gte": self.score_threshold}}
            )
            
            if not results['documents'][0]:
                return []
            
            # Format memories with metadata
            memories = []
            for i, doc in enumerate(results['documents'][0]):
                memories.append({
                    'content': doc,
                    'score': results['metadatas'][0][i].get('score', 0),
                    'distance': results['distances'][0][i] if results['distances'] else 0
                })
            
            return sorted(memories, key=lambda x: x['score'], reverse=True)
            
        except Exception as e:
            print(f"Memory retrieval error: {e}")
            return []
    
    def store_memory(self, score: float):
        """Store successful content in memory"""
        if score >= self.score_threshold and hasattr(self, 'pending_memory'):
            embedding = self.embedder.encode(self.pending_memory["content"])
            
            self.memory.add(
                embeddings=[embedding.tolist()],
                documents=[self.pending_memory["content"]],
                metadatas=[{
                    **self.pending_memory,
                    "score": score,
                    "parameters": json.dumps(self.parameters)
                }],
                ids=[f"{self.role}_{datetime.now().timestamp()}"]
            )
    
    def evolve_parameters(self, score: float, state: BlogState):
        """Adjust parameters based on performance"""
        if not self.enable_evolution:
            return
        
        # Track score history
        self.parameters["score_history"].append(score)
        self.parameters["generation_count"] += 1
        
        # Calculate running average
        recent_scores = self.parameters["score_history"][-5:]
        avg_score = sum(recent_scores) / len(recent_scores)
        
        # Adjust temperature based on performance
        learning_rate = float(os.getenv("EVOLUTION_LEARNING_RATE", 0.1))
        
        if avg_score < 6.0:
            # Poor performance: reduce randomness
            delta = -learning_rate
        elif avg_score > 8.0:
            # Good performance: increase creativity
            delta = learning_rate
        else:
            # Stable performance: minor adjustments
            delta = (7.0 - avg_score) * learning_rate * 0.5
        
        # Apply change with bounds
        old_temp = self.parameters["temperature"]
        new_temp = old_temp + delta
        new_temp = max(float(os.getenv("MIN_TEMPERATURE", 0.5)), 
                      min(float(os.getenv("MAX_TEMPERATURE", 1.0)), new_temp))
        
        self.parameters["temperature"] = new_temp
        
        # Log parameter change
        if os.getenv("SHOW_PARAMETER_CHANGES", "true").lower() == "true":
            state["parameter_updates"][self.role] = {
                "old_temperature": old_temp,
                "new_temperature": new_temp,
                "score": score,
                "avg_score": avg_score
            }
    
    @property
    @abstractmethod
    def content_key(self) -> str:
        """State key for this agent's content"""
        pass
    
    @abstractmethod
    async def generate_content(self, state: BlogState, memories: List[Dict]) -> str:
        """Generate content based on state and memories"""
        pass
```

### **3.3 Specialized Agents**

```python
# src/agents.py (continued)

class IntroAgent(BaseAgent):
    @property
    def content_key(self) -> str:
        return "intro"
    
    async def generate_content(self, state: BlogState, memories: List[Dict]) -> str:
        memory_examples = "\n\n".join([
            f"Example (score: {m['score']:.1f}):\n{m['content']}" 
            for m in memories[:2]
        ]) if memories else "No previous examples available."
        
        prompt = f"""Write an engaging introduction for a blog post about: {state['topic']}

Previous successful introductions on similar topics:
{memory_examples}

Requirements:
- 2-3 sentences
- Hook the reader immediately
- Mention the topic naturally
- Set expectations for what follows

Introduction:"""
        
        response = await self.llm.ainvoke(prompt)
        return response.content


class BodyAgent(BaseAgent):
    @property
    def content_key(self) -> str:
        return "body"
    
    async def generate_content(self, state: BlogState, memories: List[Dict]) -> str:
        # Can see the intro if it was generated
        context = f"Introduction: {state.get('intro', 'Not yet written')}"
        
        memory_examples = "\n\n".join([
            f"Example body (score: {m['score']:.1f}):\n{m['content'][:200]}..." 
            for m in memories[:2]
        ]) if memories else ""
        
        prompt = f"""Write the main body for a blog post about: {state['topic']}

{context}

{memory_examples}

Requirements:
- 3-4 paragraphs
- Informative and detailed
- Include specific examples or data
- Natural flow from the introduction

Body:"""
        
        response = await self.llm.ainvoke(prompt)
        return response.content


class ConclusionAgent(BaseAgent):
    @property
    def content_key(self) -> str:
        return "conclusion"
    
    async def generate_content(self, state: BlogState, memories: List[Dict]) -> str:
        # Can see intro and body if generated
        context = f"""
Introduction: {state.get('intro', 'Not yet written')}

Body preview: {state.get('body', 'Not yet written')[:200]}...
"""
        
        memory_examples = "\n".join([
            f"Example: {m['content']}" 
            for m in memories[:1]
        ]) if memories else ""
        
        prompt = f"""Write a conclusion for this blog post about: {state['topic']}

{context}

{memory_examples}

Requirements:
- 2-3 sentences
- Summarize key points
- End with memorable statement
- Call to action or thought to ponder

Conclusion:"""
        
        response = await self.llm.ainvoke(prompt)
        return response.content
```

### **3.4 Evaluation System**

```python
# src/evaluation.py
from typing import Dict
import re

class ContentEvaluator:
    """Multi-factor content scoring"""
    
    def __call__(self, state: BlogState) -> BlogState:
        """Score each section based on multiple factors"""
        
        scores = {
            "intro": self._score_intro(state["intro"], state["topic"]),
            "body": self._score_body(state["body"], state["topic"]),
            "conclusion": self._score_conclusion(
                state["conclusion"], 
                state["topic"], 
                state["intro"]
            )
        }
        
        state["scores"] = scores
        
        # Log scores for visualization
        state["stream_logs"].append(
            f"[Evaluator] Scores - Intro: {scores['intro']:.1f}, "
            f"Body: {scores['body']:.1f}, Conclusion: {scores['conclusion']:.1f}"
        )
        
        return state
    
    def _score_intro(self, intro: str, topic: str) -> float:
        score = 5.0  # Base score
        
        # Length check
        word_count = len(intro.split())
        if 20 <= word_count <= 60:
            score += 1.5
        
        # Topic relevance
        if topic.lower() in intro.lower():
            score += 1.5
        
        # Engagement hooks
        hooks = ["discover", "learn", "imagine", "what if", "have you ever"]
        if any(hook in intro.lower() for hook in hooks):
            score += 1.0
        
        # Question mark (engaging questions)
        if "?" in intro:
            score += 1.0
        
        return min(10.0, score)
    
    def _score_body(self, body: str, topic: str) -> float:
        score = 5.0
        
        # Length and structure
        word_count = len(body.split())
        if word_count > 150:
            score += 1.5
        
        # Paragraph structure
        paragraphs = body.split('\n\n')
        if 2 <= len(paragraphs) <= 5:
            score += 1.0
        
        # Topic coverage
        topic_words = topic.lower().split()
        topic_coverage = sum(1 for word in topic_words if word in body.lower())
        if topic_coverage >= len(topic_words) * 0.7:
            score += 1.5
        
        # Specific examples or data
        if any(char.isdigit() for char in body) or "example" in body.lower():
            score += 1.0
        
        return min(10.0, score)
    
    def _score_conclusion(self, conclusion: str, topic: str, intro: str) -> float:
        score = 5.0
        
        # Length check
        word_count = len(conclusion.split())
        if 20 <= word_count <= 50:
            score += 1.5
        
        # Summarization keywords
        summary_words = ["summary", "remember", "key", "learned", "important"]
        if any(word in conclusion.lower() for word in summary_words):
            score += 1.5
        
        # Call to action
        cta_words = ["try", "start", "begin", "explore", "consider"]
        if any(word in conclusion.lower() for word in cta_words):
            score += 1.0
        
        # Echoes intro theme
        intro_words = set(intro.lower().split())
        conclusion_words = set(conclusion.lower().split())
        if len(intro_words & conclusion_words) > 3:
            score += 1.0
        
        return min(10.0, score)
```

### **3.5 Visualization Module**

```python
# src/visualization.py
from rich.console import Console
from rich.table import Table
from rich.live import Live
from rich.layout import Layout
from rich.panel import Panel
from rich.progress import Progress, SpinnerColumn, TextColumn
from typing import Dict, List
import asyncio

class StreamVisualizer:
    """Real-time visualization of agent execution"""
    
    def __init__(self):
        self.console = Console()
        self.show_visualization = os.getenv("ENABLE_VISUALIZATION", "true").lower() == "true"
    
    def create_status_table(self, state: BlogState) -> Table:
        """Create status table for current execution"""
        table = Table(title="Agent Execution Status")
        table.add_column("Agent", style="cyan", no_wrap=True)
        table.add_column("Status", style="magenta")
        table.add_column("Memories Retrieved", style="green")
        table.add_column("Temperature", style="yellow")
        table.add_column("Score", style="blue")
        
        for role in ["intro", "body", "conclusion"]:
            memories = len(state.get("retrieved_memories", {}).get(role, []))
            param_update = state.get("parameter_updates", {}).get(role, {})
            temp = param_update.get("new_temperature", 0.7)
            score = state.get("scores", {}).get(role, 0.0)
            
            status = "‚úì Complete" if state.get(role) else "‚ü≥ Processing"
            
            table.add_row(
                role.capitalize(),
                status,
                str(memories),
                f"{temp:.2f}",
                f"{score:.1f}"
            )
        
        return table
    
    def create_memory_panel(self, state: BlogState) -> Panel:
        """Show retrieved memories"""
        memories_text = ""
        
        for role, memories in state.get("retrieved_memories", {}).items():
            if memories:
                memories_text += f"[bold]{role.upper()}:[/bold]\n"
                for i, mem in enumerate(memories[:2], 1):
                    preview = mem[:100] + "..." if len(mem) > 100 else mem
                    memories_text += f"  {i}. {preview}\n"
                memories_text += "\n"
        
        return Panel(
            memories_text or "No memories retrieved yet",
            title="Retrieved Memories",
            border_style="green"
        )
    
    def create_evolution_panel(self, state: BlogState) -> Panel:
        """Show parameter evolution"""
        evolution_text = ""
        
        for role, updates in state.get("parameter_updates", {}).items():
            if updates:
                old_t = updates['old_temperature']
                new_t = updates['new_temperature']
                score = updates['score']
                avg = updates['avg_score']
                
                change = "‚Üë" if new_t > old_t else "‚Üì" if new_t < old_t else "‚Üí"
                
                evolution_text += (
                    f"[bold]{role.upper()}:[/bold]\n"
                    f"  Temperature: {old_t:.2f} {change} {new_t:.2f}\n"
                    f"  Last Score: {score:.1f} | Avg: {avg:.1f}\n\n"
                )
        
        return Panel(
            evolution_text or "No parameter updates yet",
            title="Parameter Evolution",
            border_style="yellow"
        )
    
    async def stream_execution(self, state_updates):
        """Display real-time execution updates"""
        if not self.show_visualization:
            return
        
        layout = Layout()
        layout.split_column(
            Layout(name="status", size=10),
            Layout(name="memories", size=10),
            Layout(name="evolution", size=8),
            Layout(name="logs", size=5)
        )
        
        with Live(layout, refresh_per_second=4) as live:
            async for state in state_updates:
                layout["status"].update(self.create_status_table(state))
                layout["memories"].update(self.create_memory_panel(state))
                layout["evolution"].update(self.create_evolution_panel(state))
                
                # Show latest logs
                logs = state.get("stream_logs", [])[-5:]
                logs_text = "\n".join(logs)
                layout["logs"].update(Panel(logs_text, title="Activity Log"))
```

### **3.6 Main Graph Construction**

```python
# main.py
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolExecutor
from langgraph.checkpoint.memory import MemorySaver
import chromadb
import asyncio
from datetime import datetime
import uuid
from src.state import BlogState
from src.agents import IntroAgent, BodyAgent, ConclusionAgent
from src.evaluation import ContentEvaluator
from src.visualization import StreamVisualizer
import os

class HVASMiniPipeline:
    """Main pipeline orchestrator"""
    
    def __init__(self):
        # Initialize ChromaDB
        self.chroma = chromadb.Client()
        
        # Initialize agents
        self.agents = {
            "intro": IntroAgent("intro", self.chroma),
            "body": BodyAgent("body", self.chroma),
            "conclusion": ConclusionAgent("conclusion", self.chroma)
        }
        
        # Initialize evaluator and visualizer
        self.evaluator = ContentEvaluator()
        self.visualizer = StreamVisualizer()
        
        # Build graph
        self.app = self._build_graph()
    
    def _build_graph(self) -> StateGraph:
        """Construct LangGraph workflow with parallel execution"""
        
        workflow = StateGraph(BlogState)
        
        # Add nodes
        workflow.add_node("intro", self.agents["intro"])
        workflow.add_node("body", self.agents["body"])
        workflow.add_node("conclusion", self.agents["conclusion"])
        workflow.add_node("evaluate", self.evaluator)
        workflow.add_node("evolve", self._evolution_node)
        
        # Set entry point
        workflow.set_entry_point("intro")
        
        # Define edges for parallel execution where possible
        workflow.add_edge("intro", "body")
        workflow.add_edge("body", "conclusion")
        workflow.add_edge("conclusion", "evaluate")
        workflow.add_edge("evaluate", "evolve")
        workflow.add_edge("evolve", END)
        
        # Compile with memory for checkpointing
        memory = MemorySaver()
        return workflow.compile(checkpointer=memory)
    
    async def _evolution_node(self, state: BlogState) -> BlogState:
        """Apply evolution and store memories"""
        
        for role, agent in self.agents.items():
            score = state["scores"].get(role, 0)
            
            # Store memory if successful
            agent.store_memory(score)
            
            # Evolve parameters
            agent.evolve_parameters(score, state)
        
        state["stream_logs"].append(
            f"[Evolution] Memories stored, parameters updated"
        )
        
        return state
    
    async def generate(self, topic: str) -> Dict:
        """Generate blog post with streaming visualization"""
        
        # Initialize state
        initial_state = BlogState(
            topic=topic,
            intro="",
            body="",
            conclusion="",
            scores={},
            retrieved_memories={},
            parameter_updates={},
            generation_id=str(uuid.uuid4()),
            timestamp=datetime.now().isoformat(),
            stream_logs=[]
        )
        
        # Configure streaming
        config = {
            "configurable": {
                "thread_id": f"blog_{topic.replace(' ', '_')}_{datetime.now().timestamp()}"
            }
        }
        
        # Stream execution with visualization
        final_state = None
        
        if os.getenv("ENABLE_VISUALIZATION", "true").lower() == "true":
            async for event in self.app.astream(initial_state, config, stream_mode="values"):
                await self.visualizer.stream_execution([event])
                final_state = event
        else:
            # Run without visualization
            final_state = await self.app.ainvoke(initial_state, config)
        
        return {
            "content": {
                "intro": final_state["intro"],
                "body": final_state["body"],
                "conclusion": final_state["conclusion"]
            },
            "scores": final_state["scores"],
            "memories": {
                role: self.agents[role].memory.count() 
                for role in self.agents
            },
            "parameters": {
                role: agent.parameters 
                for role, agent in self.agents.items()
            }
        }

async def main():
    """Demo execution"""
    pipeline = HVASMiniPipeline()
    
    # Test topics to show learning
    topics = [
        "introduction to machine learning",
        "machine learning applications",  # Similar - will use memories
        "python programming basics",
        "python for data science",  # Similar - will use memories
        "artificial intelligence ethics"
    ]
    
    for i, topic in enumerate(topics, 1):
        print(f"\n{'='*60}")
        print(f"Generation {i}: {topic}")
        print('='*60)
        
        result = await pipeline.generate(topic)
        
        # Display results
        print("\nüìù GENERATED CONTENT:")
        print(f"\nIntro: {result['content']['intro']}")
        print(f"\nBody: {result['content']['body'][:300]}...")
        print(f"\nConclusion: {result['content']['conclusion']}")
        
        print("\nüìä SCORES:")
        for role, score in result['scores'].items():
            print(f"  {role}: {score:.1f}/10")
        
        print("\nüß† MEMORY STATUS:")
        for role, count in result['memories'].items():
            print(f"  {role}: {count} memories stored")
        
        print("\nüîß CURRENT PARAMETERS:")
        for role, params in result['parameters'].items():
            temp = params['temperature']
            gens = params['generation_count']
            print(f"  {role}: temp={temp:.2f}, generations={gens}")
        
        # Brief pause between generations
        await asyncio.sleep(2)

if __name__ == "__main__":
    asyncio.run(main())
```

---

## **4. Key Features**

### **4.1 Streaming & Async Execution**
- Built-in LangGraph streaming with `astream()` method
- Async nodes for parallel agent execution
- Real-time state updates during generation

### **4.2 Visualization Features**
- Live execution status table
- Memory retrieval display
- Parameter evolution tracking
- Activity log stream
- Rich terminal UI with panels and tables

### **4.3 Parameter Evolution**
- Configurable via `.env` (can be disabled)
- Temperature adjustment based on rolling average
- Bounded evolution (MIN/MAX_TEMPERATURE)
- Per-agent tracking and history

### **4.4 Memory System**
- Per-agent ChromaDB collections
- Score-based filtering
- Semantic similarity retrieval
- Metadata tracking for analysis

---

## **5. Configuration Guide**

### **Quick Setup**
```bash
# Install
pip install -r requirements.txt

# Configure
cp .env.example .env
# Edit .env with your API key

# Run
python main.py
```

### **Disable Features**
```bash
# Turn off parameter evolution
ENABLE_PARAMETER_EVOLUTION=false

# Turn off visualization
ENABLE_VISUALIZATION=false

# Turn off memory retrieval display
SHOW_MEMORY_RETRIEVAL=false
```

---

## **6. Expected Output**

```
Generation 1: introduction to machine learning
============================================================
[Visualizer shows live execution...]

üìù GENERATED CONTENT:
Intro: Have you ever wondered how computers learn from data?...

üìä SCORES:
  intro: 7.5/10
  body: 8.0/10
  conclusion: 7.0/10

üß† MEMORY STATUS:
  intro: 1 memories stored
  body: 1 memories stored
  conclusion: 1 memories stored

üîß CURRENT PARAMETERS:
  intro: temp=0.72, generations=1
  body: temp=0.74, generations=1
  conclusion: temp=0.68, generations=1
```

This implementation demonstrates all core HVAS concepts with production-ready patterns using LangGraph's native capabilities for streaming, async execution, and state management.
